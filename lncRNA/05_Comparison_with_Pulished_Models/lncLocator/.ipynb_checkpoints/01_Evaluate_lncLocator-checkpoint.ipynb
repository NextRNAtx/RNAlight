{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bash\n",
    "# To get lncRNA_sublocation_TestSet_with_transcript_id.fa\n",
    "    Instance of lncRNA_sublocation_TestSet_with_transcript_id.fa\n",
    "    \n",
    "    >ENST00000376398_1\n",
    "    TGAAATAGGAGCCAAGGTATGCTATGAGCCAAGGATTATGAGTAATCCAGTTTTGTGCAC\n",
    "    TTTAAGCCATTTGAAAAACAGAAAAGCAAAACAACAAAATAATTTTTAAGAAATTGAATA\n",
    "    \n",
    "    ENST00000376398 and 1 are the ensembl id and tag of lncRNA, respectively.(0:Cytosol, 1:Nuclear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    " sed '1d' ../../Datasets/lncRNA_sublocation_TestSet.tsv \\\n",
    " | awk -v OFS='\\t' '{print $1\"_\"$4,$3}' \\\n",
    " | seqkit tab2fx > lncRNA_sublocation_TestSet.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submmit fasta file to the lncLocator\n",
    "    (http://www.csbio.sjtu.edu.cn/bioinf/lncLocator/#) \n",
    "    You can get the prediction (result.fasta)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python\n",
    "# To evaluate performance of lncLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance of model\n",
    "def evaluate_performance(y_test, y_pred, y_prob):\n",
    "    # AUROC\n",
    "    auroc = metrics.roc_auc_score(y_test,y_prob)\n",
    "    auroc_curve = metrics.roc_curve(y_test, y_prob)\n",
    "    # AUPRC\n",
    "    auprc=metrics.average_precision_score(y_test, y_prob) \n",
    "    auprc_curve=metrics.precision_recall_curve(y_test, y_prob)\n",
    "    #Accuracy\n",
    "    accuracy=metrics.accuracy_score(y_test,y_pred) \n",
    "    #MCC\n",
    "    mcc=metrics.matthews_corrcoef(y_test,y_pred)\n",
    "    \n",
    "    recall=metrics.recall_score(y_test, y_pred)\n",
    "    precision=metrics.precision_score(y_test, y_pred)\n",
    "    f1=metrics.f1_score(y_test, y_pred)\n",
    "    class_report=metrics.classification_report(y_test, y_pred,target_names = [\"control\",\"case\"])\n",
    "\n",
    "    model_perf = {\"auroc\":auroc,\"auroc_curve\":auroc_curve,\n",
    "                  \"auprc\":auprc,\"auprc_curve\":auprc_curve,\n",
    "                  \"accuracy\":accuracy, \"mcc\": mcc,\n",
    "                  \"recall\":recall,\"precision\":precision,\"f1\":f1,\n",
    "                  \"class_report\":class_report}\n",
    "        \n",
    "    return model_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output result of evaluation\n",
    "def eval_output(model_perf,path):\n",
    "    with open(os.path.join(path,\"Evaluate_Result_TestSet.txt\"),'w') as f:\n",
    "        f.write(\"AUROC=%s\\tAUPRC=%s\\tAccuracy=%s\\tMCC=%s\\tRecall=%s\\tPrecision=%s\\tf1_score=%s\\n\" %\n",
    "               (model_perf[\"auroc\"],model_perf[\"auprc\"],model_perf[\"accuracy\"],model_perf[\"mcc\"],model_perf[\"recall\"],model_perf[\"precision\"],model_perf[\"f1\"]))\n",
    "        f.write(\"\\n######NOTE#######\\n\")\n",
    "        f.write(\"#According to help_documentation of sklearn.metrics.classification_report:in binary classification, recall of the positive class is also known as sensitivity; recall of the negative class is specificity#\\n\\n\")\n",
    "        f.write(model_perf[\"class_report\"])\n",
    "        roc_auc = model_perf[\"auroc\"]\n",
    "    # AUROC info\n",
    "    fpr,tpr,threshold = model_perf[\"auroc_curve\"]\n",
    "    #return AUROC info\n",
    "    temp_df = pd.DataFrame({\"FPR\":fpr,\"TPR\":tpr})\n",
    "    temp_df.to_csv(os.path.join(path,\"AUROC_info.txt\"),header = True,index = False, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"ensemble_transcript_id\" , \"True_Label\" , \"Pre_Label\", \"Cyto_Score\" , \"Nuc_Score\"]\n",
    "df_dict = {\"ensemble_transcript_id\" : [], \"True_Label\" : [], \"Pre_Label\" : [], \"Cyto_Score\" : [], \"Nuc_Score\" : []}\n",
    "input_file = \"./result.fasta\"\n",
    "with open(input_file,'r') as f1:\n",
    "    fasta_result = f1.readlines()\n",
    "    lnc_num = int(len(fasta_result)/6)\n",
    "    for i in range(lnc_num):\n",
    "        index = 6 * i\n",
    "        #Seq_Name and True_Label\n",
    "        fasta_name = fasta_result[index].strip().split('\\t')[0]\n",
    "        df_dict[\"ensemble_transcript_id\"].append(fasta_name.split('_')[0][1:])\n",
    "        df_dict[\"True_Label\"].append(fasta_name.split('_')[1])\n",
    "        #Nuc_Score\n",
    "        nuc_score = float(fasta_result[index + 2].strip().split('\\t')[1])\n",
    "        df_dict[\"Nuc_Score\"].append(nuc_score)\n",
    "        #Cyto_Score\n",
    "        cyto_score = 1 - nuc_score\n",
    "        df_dict[\"Cyto_Score\"].append(cyto_score)\n",
    "        #Pre_Label\n",
    "        if cyto_score >= nuc_score:\n",
    "            df_dict[\"Pre_Label\"].append(0)\n",
    "        else:\n",
    "            df_dict[\"Pre_Label\"].append(1)\n",
    "            \n",
    "outcome = pd.DataFrame(df_dict,columns = columns)\n",
    "outcome[[\"True_Label\"]] = outcome[[\"True_Label\"]].astype(int)\n",
    "outcome[[\"Pre_Label\"]] = outcome[[\"Pre_Label\"]].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output performance of lncLocator\n",
    "path = \"./Evaluation_Result\"\n",
    "if not (os.path.exists(path)):\n",
    "    os.mkdir(path)\n",
    "model_perf = evaluate_performance(outcome[\"True_Label\"],outcome[\"Pre_Label\"],outcome[\"Nuc_Score\"])\n",
    "eval_output(model_perf,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/picb/rnomics3/yuanguohua/anaconda3/envs/pytorch/lib/python3.6/site-packages/pandas/core/frame.py:2746: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if isinstance(loc, (slice, Series, np.ndarray, Index)):\n"
     ]
    }
   ],
   "source": [
    "outcome_df = outcome.iloc[:,[0,1,2]]\n",
    "outcome_df.rename(columns={\"True_Label\":\"tag\",\"Pre_Label\":\"predict_label\"},inplace = True)\n",
    "outcome_df.to_csv(os.path.join(path,\"lncRNA_sublocation_TestSet_lncLocator_predict.tsv\"),sep = '\\t',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
