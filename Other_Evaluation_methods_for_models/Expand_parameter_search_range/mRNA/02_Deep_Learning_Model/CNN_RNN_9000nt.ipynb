{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python import and define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "###  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python import\n",
    "import os\n",
    "import argparse\n",
    "import collections\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import  sklearn.preprocessing as preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, backend\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from tfdeterminism import patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-RNN Model\n",
    "def create_CCR_model(filter_num,h_units):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    #Conv1d layer1\n",
    "    model.add(layers.Conv1D(filters = filter_num,kernel_size = 12,strides = 1,padding = \"valid\",input_shape = (9000,4),activation = \"relu\"))\n",
    "\n",
    "    #Conv1d layer2\n",
    "    model.add(layers.Conv1D(filters = int(filter_num/2),kernel_size = 12,strides = 1,padding = \"valid\",activation = \"relu\"))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    #Max_pooling layer\n",
    "    model.add(layers.MaxPooling1D(pool_size =4, strides =4))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    #BiLSTM layer\n",
    "    model.add(layers.Bidirectional(layers.LSTM(h_units)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    #Flatten\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    #Dense Layer\n",
    "    model.add(layers.Dense(units = 1024, activation = \"relu\"))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    #Output\n",
    "    model.add(layers.Dense(units = 2, activation = \"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether in jupyter notebook\n",
    "def isnotebook() -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the current execution environment is a jupyter notebook\n",
    "    https://stackoverflow.com/questions/15411967/how-can-i-check-if-code-is-executed-in-the-ipython-notebook\n",
    "    \"\"\"\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get filepath of input and output\n",
    "def get_filepath():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--training\",dest = \"training\", help = \"TrainingSet file\")\n",
    "    parser.add_argument(\"--test\",dest = \"test\",default = 20,help = \"TestSet file\")\n",
    "    parser.add_argument(\"-O\",\"-o\",\"--outputdir\",dest = \"outputdir\",help = \"Output dir of models\")\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding sequence to 9000nt\n",
    "def padding_truncate_seq(seq):\n",
    "    length = len(seq)  \n",
    "    if length > 9000: \n",
    "        seq = seq[:9000]   # trancate seq to 9000nt\n",
    "    else:\n",
    "        seq = seq + (9000 -length)*'N'  #padding N after seq to 9000nt\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensor\n",
    "def convert2tensor(data):\n",
    "    NT_dict = {'A':np.array([1,0,0,0]),'T':np.array([0,1,0,0]),'G':np.array([0,0,1,0]),'C':np.array([0,0,0,1]),'N':np.array([0,0,0,0])}\n",
    "    label_dict = {'0': 0, '1':1}\n",
    "    \n",
    "    seq = tf.convert_to_tensor(data[\"seq\"].apply(lambda x:np.array([ NT_dict[i] for i in x],dtype=np.float32)))\n",
    "    label = tf.convert_to_tensor(data[\"label\"].apply(lambda x:np.array(label_dict[str(x)],dtype=np.float32)))\n",
    "    \n",
    "    Set = {\"seq\":seq,\"label\":label}\n",
    "    \n",
    "    return Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate performance of model\n",
    "def evaluate_performance(y_test, y_pred, y_prob):\n",
    "    # AUROC\n",
    "    auroc = metrics.roc_auc_score(y_test,y_prob)\n",
    "    auroc_curve = metrics.roc_curve(y_test, y_prob)\n",
    "    # AUPRC\n",
    "    auprc=metrics.average_precision_score(y_test, y_prob) \n",
    "    auprc_curve=metrics.precision_recall_curve(y_test, y_prob)\n",
    "    #Accuracy\n",
    "    accuracy=metrics.accuracy_score(y_test,y_pred) \n",
    "    #MCC\n",
    "    mcc=metrics.matthews_corrcoef(y_test,y_pred)\n",
    "    \n",
    "    recall=metrics.recall_score(y_test, y_pred)\n",
    "    precision=metrics.precision_score(y_test, y_pred)\n",
    "    f1=metrics.f1_score(y_test, y_pred)\n",
    "    class_report=metrics.classification_report(y_test, y_pred,target_names = [\"control\",\"case\"])\n",
    "\n",
    "    model_perf = {\"auroc\":auroc,\"auroc_curve\":auroc_curve,\n",
    "                  \"auprc\":auprc,\"auprc_curve\":auprc_curve,\n",
    "                  \"accuracy\":accuracy, \"mcc\": mcc,\n",
    "                  \"recall\":recall,\"precision\":precision,\"f1\":f1,\n",
    "                  \"class_report\":class_report}\n",
    "        \n",
    "    return model_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output result of evaluation\n",
    "def eval_output(model_perf,path):\n",
    "    with open(os.path.join(path,\"Evaluate_Result_TestSet.txt\"),'w') as f:\n",
    "        f.write(\"AUROC=%s\\tAUPRC=%s\\tAccuracy=%s\\tMCC=%s\\tRecall=%s\\tPrecision=%s\\tf1_score=%s\\n\" %\n",
    "               (model_perf[\"auroc\"],model_perf[\"auprc\"],model_perf[\"accuracy\"],model_perf[\"mcc\"],model_perf[\"recall\"],model_perf[\"precision\"],model_perf[\"f1\"]))\n",
    "        f.write(\"\\n######NOTE#######\\n\")\n",
    "        f.write(\"#According to help_documentation of sklearn.metrics.classification_report:in binary classification, recall of the positive class is also known as sensitivity; recall of the negative class is specificity#\\n\\n\")\n",
    "        f.write(model_perf[\"class_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUROC of model\n",
    "def plot_AUROC(model_perf,path):\n",
    "    #get AUROC,FPR,TPR and threshold\n",
    "    roc_auc = model_perf[\"auroc\"]\n",
    "    fpr,tpr,threshold = model_perf[\"auroc_curve\"]\n",
    "    #return AUROC info\n",
    "    temp_df = pd.DataFrame({\"FPR\":fpr,\"TPR\":tpr})\n",
    "    temp_df.to_csv(os.path.join(path,\"AUROC_info.txt\"),header = True,index = False, sep = '\\t')\n",
    "    #plot\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='AUROC (area = %0.2f)' % roc_auc) \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"AUROC of Models\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(os.path.join(path,\"AUROC_TestSet.pdf\"),format = \"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/picb/rnomics3/yuanguohua/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "TensorFlow version 2.0.0 has been patched using tfdeterminism version 0.3.0\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "CONFIG = ConfigProto()\n",
    "CONFIG.gpu_options.allow_growth = True\n",
    "SESSION = InteractiveSession(config=CONFIG)\n",
    "TF_DETERMINISTIC_OPS=1\n",
    "# hdf5 \n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "# Random seed\n",
    "patch()\n",
    "SEED = 100\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "# Hyper-parameters\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH = 32\n",
    "SHUFFLE = True\n",
    "EPOCH = 100\n",
    "LOSS = \"sparse_categorical_crossentropy\"\n",
    "METRICS = [\"accuracy\",\"sparse_categorical_crossentropy\"]\n",
    "PATIENCE = 10\n",
    "# previous_filters_num_list: [32,64,128]\n",
    "filters_num_list = [16,32,64,128,256]\n",
    "# previous_h_units_list: [16,32,64]\n",
    "h_units_list = [16,32,64,128,256]\n",
    "\n",
    "# Index of model performance\n",
    "ModelPerf = collections.namedtuple('ModelPerf',\n",
    "                                   ['auroc', 'auroc_curve', 'auprc', 'auprc_curve', 'accuracy', \n",
    "                                    'mcc','recall', 'precision', 'f1', 'class_report','ce_loss'])\n",
    "\n",
    "# Output direction\n",
    "output_dir = \"./01_DL_Model_Output/CNN_RNN_9000nt_Model_Output\"\n",
    "if not (os.path.exists(output_dir)):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input_file path\n",
    "if isnotebook():\n",
    "    training_f = \"/data/rnomics8/yuanguohua/RNAlight_Private/mRNA/03_Model_Construction/mRNA_sublocation_TrainingSet.tsv\"\n",
    "    test_f = \"/data/rnomics8/yuanguohua/RNAlight_Private/mRNA/03_Model_Construction/mRNA_sublocation_TestSet.tsv\"\n",
    "else:\n",
    "    args = get_filepath()\n",
    "    training_f = args.training\n",
    "    test_f = args.test\n",
    "    output_dir = args.outputdir\n",
    "# Load data\n",
    "dataset_training = pd.read_csv(training_f,sep='\\t',index_col = False)    #4662  \n",
    "dataset_test = pd.read_csv(test_f,sep='\\t',index_col = False)    #518\n",
    "# Fix seq to 4000nt\n",
    "dataset_training[\"cdna\"] = dataset_training[\"cdna\"].apply(lambda x:padding_truncate_seq(x))\n",
    "dataset_test[\"cdna\"] = dataset_test[\"cdna\"].apply(lambda x:padding_truncate_seq(x))\n",
    "# Format :[seq] [label]\n",
    "dataset_training = dataset_training.iloc[:,2:4]\n",
    "dataset_training.columns = [\"seq\",\"label\"]\n",
    "dataset_test = dataset_test.iloc[:,2:4]\n",
    "dataset_test.columns = [\"seq\",\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimize hyperparameters by five-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataframe can store the highest mean of auroc among five-fold cross validations with each combination of hyperparameters\n",
    "comb_auroc = pd.DataFrame(columns = [\"filter_num\",\"h_units\",\"auroc_on_CV\"])\n",
    "\n",
    "# Optimize hyperparameters about model architechture\n",
    "for filter_num in filters_num_list:\n",
    "    for h_units in h_units_list:\n",
    "        # Five-fold cross validation\n",
    "        cvscore_auroc_iter = []\n",
    "        kfold = KFold(n_splits=5, random_state=SEED, shuffle=True)\n",
    "        for train_index, validate_index in kfold.split(dataset_training):\n",
    "            # Get cross validation set\n",
    "            train,validate = dataset_training.loc[train_index,],dataset_training.loc[validate_index,]\n",
    "            TrainingSet = convert2tensor(train)\n",
    "            ValidationSet = convert2tensor(validate)\n",
    "\n",
    "            # Train model\n",
    "            Net = create_CCR_model(filter_num = filter_num, h_units = h_units)\n",
    "            callbacks = [keras.callbacks.EarlyStopping(monitor = \"val_accuracy\",patience=PATIENCE, \n",
    "                                                       min_delta=1e-3,mode = \"auto\",restore_best_weights = True)]\n",
    "            Net.compile(optimizer = OPTIMIZER, loss = LOSS, metrics = METRICS)\n",
    "            Net_history = Net.fit(TrainingSet[\"seq\"],\n",
    "                                  TrainingSet[\"label\"],\n",
    "                                  batch_size = BATCH,\n",
    "                                  epochs = EPOCH,\n",
    "                                  shuffle = SHUFFLE,\n",
    "                                  callbacks = callbacks,\n",
    "                                  validation_data = (ValidationSet[\"seq\"],ValidationSet[\"label\"]),\n",
    "                                  verbose = 0)\n",
    "\n",
    "            # Get the auroc of the model with maximum accuracy on validation set per fold\n",
    "            cvscore_auroc_iter.append(Net_history.history[\"val_accuracy\"][-(PATIENCE+1)])\n",
    "            \n",
    "        # Add to the dataframe    \n",
    "        comb_auroc.loc[len(comb_auroc)] = [filter_num,h_units,cvscore_auroc_iter]\n",
    "\n",
    "        \n",
    "# Choose the optimal combnation of hyperparameters\n",
    "comb_auroc[\"mean_auroc_on_CV\"] = comb_auroc[\"auroc_on_CV\"].apply(np.mean)\n",
    "optimal_comb_index = comb_auroc[\"mean_auroc_on_CV\"].argmax()\n",
    "optimal_filter_num = comb_auroc[\"filter_num\"].loc[optimal_comb_index]\n",
    "optimal_h_units = comb_auroc[\"h_units\"].loc[optimal_comb_index]\n",
    "\n",
    "# Output result of cross validation\n",
    "comb_auroc.to_csv(os.path.join(output_dir,\"CNN_RNN_CV_result.tsv\"),sep = '\\t',header = True,index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Refit model with the optimal hyperparameters and whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole training set\n",
    "Whole_training = convert2tensor(dataset_training)\n",
    "\n",
    "# Refit model\n",
    "logdir = os.path.join(output_dir,\"log\")\n",
    "Net_final = create_CCR_model(filter_num = optimal_filter_num , h_units = optimal_h_units)\n",
    "callbacks_final = [keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "                   keras.callbacks.EarlyStopping(monitor = \"val_accuracy\",patience=PATIENCE,restore_best_weights = True),\n",
    "                   keras.callbacks.ModelCheckpoint(filepath = os.path.join(output_dir, \"best_model.h5\"), monitor=\"val_accuracy\",\n",
    "                                                   save_best_only=True,mode = \"auto\")]\n",
    "Net_final.compile(optimizer = OPTIMIZER, loss = LOSS, metrics = METRICS)\n",
    "Net_final_history = Net_final.fit(Whole_training[\"seq\"],\n",
    "                                  Whole_training[\"label\"],\n",
    "                                  batch_size = BATCH,\n",
    "                                  epochs = EPOCH,\n",
    "                                  shuffle = SHUFFLE,\n",
    "                                  callbacks = callbacks_final,\n",
    "                                  validation_split = 0.1,\n",
    "                                  verbose = 2)\n",
    "Net_final.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate model performance on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole test set\n",
    "TestSet = convert2tensor(dataset_test)\n",
    "\n",
    "# Load best model\n",
    "best_model =  tf.keras.models.load_model(os.path.join(output_dir,\"best_model.h5\"))\n",
    "# Predict\n",
    "prediction = best_model.predict(TestSet[\"seq\"])\n",
    "# Evaluate\n",
    "pre_label = np.argmax(prediction,axis=1)\n",
    "true_label = TestSet[\"label\"].numpy()\n",
    "posi_prob = prediction[:,1]\n",
    "model_perf = evaluate_performance(true_label,pre_label,posi_prob)\n",
    "\n",
    "# Plot AUROC\n",
    "plot_AUROC(model_perf,output_dir)\n",
    "# Output performance\n",
    "eval_output(model_perf,output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
