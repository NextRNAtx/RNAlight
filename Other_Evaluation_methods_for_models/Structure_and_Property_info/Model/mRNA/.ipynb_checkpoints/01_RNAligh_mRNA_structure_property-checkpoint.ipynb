{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python import and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python import\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import collections\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether in jupyter notebook\n",
    "def isnotebook() -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the current execution environment is a jupyter notebook\n",
    "    https://stackoverflow.com/questions/15411967/how-can-i-check-if-code-is-executed-in-the-ipython-notebook\n",
    "    \"\"\"\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get filepath of input and output\n",
    "def get_filepath():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-C\",\"-c\",\"--cyto\",dest = \"cytosol\", help = \"Cytosol instances file\")\n",
    "    parser.add_argument(\"-N\",\"-n\",\"--nuc\",dest = \"nuclear\",help = \"Nuclear instances file\")\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of k-mer in each RNA sequence\n",
    "# k-mer was normalized by total k-mer count of each RNA sequence\n",
    "def _count_kmer(Dataset,k): # k = 3,4,5\n",
    "    \n",
    "    # copy dataset\n",
    "    dataset = copy.deepcopy(Dataset)\n",
    "    # alphbet of nucleotide\n",
    "    nucleotide = ['A','C','G','T']\n",
    "    \n",
    "    # generate k-mers\n",
    "    #  k == 5:\n",
    "    five = list(itertools.product(nucleotide,repeat=5))\n",
    "    pentamer = []\n",
    "    for n in five:\n",
    "        pentamer.append(\"\".join(n))\n",
    "    \n",
    "    #  k == 4:\n",
    "    four = list(itertools.product(nucleotide,repeat=4))\n",
    "    tetramer = []\n",
    "    for n in four:\n",
    "        tetramer.append(\"\".join(n))\n",
    "\n",
    "    # k == 3:\n",
    "    three = list(itertools.product(nucleotide,repeat=3))\n",
    "    threemer = []\n",
    "    for n in three:\n",
    "        threemer.append(\"\".join(n))\n",
    "    \n",
    "    # input features can be combinations of diffrent k values\n",
    "    if k == 34:\n",
    "        table_kmer = dict.fromkeys(threemer,0)\n",
    "        table_kmer.update(dict.fromkeys(tetramer,0))\n",
    "    if k == 45:\n",
    "        table_kmer = dict.fromkeys(tetramer,0)\n",
    "        table_kmer.update(dict.fromkeys(pentamer,0))\n",
    "    if k == 345:\n",
    "        table_kmer = dict.fromkeys(threemer,0)\n",
    "        table_kmer.update(dict.fromkeys(tetramer,0))\n",
    "        table_kmer.update(dict.fromkeys(pentamer,0))\n",
    "\n",
    "    # count k-mer for each sequence\n",
    "    for mer in table_kmer.keys():\n",
    "        table_kmer[mer] = dataset[\"cdna\"].apply(lambda x : x.count(mer))\n",
    "    \n",
    "    # for k-mer raw count without normalization, index: nuc:1 or cyto:0\n",
    "    rawcount_kmer_df = pd.DataFrame(table_kmer)\n",
    "    df1_rawcount = pd.concat([rawcount_kmer_df,dataset[\"ensembl_transcript_id\"]],axis = 1)\n",
    "    df1_rawcount.index = dataset[\"tag\"]\n",
    "\n",
    "    # for k-mer frequency with normalization , index: nuc:1 or cyto:0\n",
    "    freq_kmer_df = rawcount_kmer_df.apply(lambda x: x/x.sum(),axis=1)\n",
    "    df1 = pd.concat([freq_kmer_df,dataset[\"ensembl_transcript_id\"]],axis = 1)\n",
    "    df1.index = dataset[\"tag\"]\n",
    "\n",
    "    return df1,df1_rawcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate performance of model\n",
    "def evaluate_performance(y_test, y_pred, y_prob):\n",
    "    # AUROC\n",
    "    auroc = metrics.roc_auc_score(y_test,y_prob)\n",
    "    auroc_curve = metrics.roc_curve(y_test, y_prob)\n",
    "    # AUPRC\n",
    "    auprc=metrics.average_precision_score(y_test, y_prob) \n",
    "    auprc_curve=metrics.precision_recall_curve(y_test, y_prob)\n",
    "    #Accuracy\n",
    "    accuracy=metrics.accuracy_score(y_test,y_pred) \n",
    "    #MCC\n",
    "    mcc=metrics.matthews_corrcoef(y_test,y_pred)\n",
    "    \n",
    "    recall=metrics.recall_score(y_test, y_pred)\n",
    "    precision=metrics.precision_score(y_test, y_pred)\n",
    "    f1=metrics.f1_score(y_test, y_pred)\n",
    "    class_report=metrics.classification_report(y_test, y_pred,target_names = [\"control\",\"case\"])\n",
    "\n",
    "    model_perf = {\"auroc\":auroc,\"auroc_curve\":auroc_curve,\n",
    "                  \"auprc\":auprc,\"auprc_curve\":auprc_curve,\n",
    "                  \"accuracy\":accuracy, \"mcc\": mcc,\n",
    "                  \"recall\":recall,\"precision\":precision,\"f1\":f1,\n",
    "                  \"class_report\":class_report}\n",
    "        \n",
    "    return model_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output result of evaluation\n",
    "def eval_output(model_perf,path):\n",
    "    with open(os.path.join(path,\"Evaluate_Result_TestSet.txt\"),'w') as f:\n",
    "        f.write(\"AUROC=%s\\tAUPRC=%s\\tAccuracy=%s\\tMCC=%s\\tRecall=%s\\tPrecision=%s\\tf1_score=%s\\n\" %\n",
    "               (model_perf[\"auroc\"],model_perf[\"auprc\"],model_perf[\"accuracy\"],model_perf[\"mcc\"],model_perf[\"recall\"],model_perf[\"precision\"],model_perf[\"f1\"]))\n",
    "        f.write(\"\\n######NOTE#######\\n\")\n",
    "        f.write(\"#According to help_documentation of sklearn.metrics.classification_report:in binary classification, recall of the positive class is also known as sensitivity; recall of the negative class is specificity#\\n\\n\")\n",
    "        f.write(model_perf[\"class_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUROC of model\n",
    "def plot_AUROC(model_perf,path):\n",
    "    #get AUROC,FPR,TPR and threshold\n",
    "    roc_auc = model_perf[\"auroc\"]\n",
    "    fpr,tpr,threshold = model_perf[\"auroc_curve\"]\n",
    "    #return AUROC info\n",
    "    temp_df = pd.DataFrame({\"FPR\":fpr,\"TPR\":tpr})\n",
    "    temp_df.to_csv(os.path.join(path,\"AUROC_info.txt\"),header = True,index = False, sep = '\\t')\n",
    "    #plot\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='AUROC (area = %0.2f)' % roc_auc) \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"AUROC of Models\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(os.path.join(path,\"AUROC_TestSet.pdf\"),format = \"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "###   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Configution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "SEED = 100\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# Output dir\n",
    "output_dir = \"01_RNAlight_Model_Output\"\n",
    "if not (os.path.exists(output_dir)):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "###   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************** Start Data Processing **************************\n",
      "\n",
      "********************** Finished Data Processing ***********************\n",
      "CPU times: user 1min 19s, sys: 434 ms, total: 1min 20s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"\\n********************** Start Data Processing **************************\")\n",
    "\n",
    "# Load dataset files\n",
    "Training_set_sequence = pd.read_csv(\"/data/rnomics8/yuanguohua/RNAlight_Private/mRNA/03_Model_Construction/mRNA_sublocation_TrainingSet.tsv\",sep='\\t')\n",
    "Training_set_structure = pd.read_csv(\"../../Structure_Dataset/mRNA/mRNA_sublocation_TrainingSet_structure.txt\",sep='\\t')\n",
    "Training_set_property = pd.read_csv(\"../../Property_Dataset/mRNA/mRNA_sublocation_TrainingSet_biochemical.txt\",sep='\\t')\n",
    "\n",
    "Test_set_sequence = pd.read_csv(\"/data/rnomics8/yuanguohua/RNAlight_Private/mRNA/03_Model_Construction/mRNA_sublocation_TestSet.tsv\",sep='\\t')\n",
    "Test_set_structure = pd.read_csv(\"../../Structure_Dataset/mRNA/mRNA_sublocation_TestSet_structure.txt\",sep='\\t')\n",
    "Test_set_property = pd.read_csv(\"../../Property_Dataset/mRNA/mRNA_sublocation_TestSet_biochemical.txt\",sep='\\t')\n",
    "\n",
    "\n",
    "# k = 3,4,5 count the normalized and raw count of kmer\n",
    "Train_df_kmer_345,Train_df_kmer_345_rawcount = _count_kmer(Training_set_sequence,345)\n",
    "del Train_df_kmer_345['ensembl_transcript_id']\n",
    "# Structure info\n",
    "Train_df_strcutrue  = Training_set_structure.iloc[:,2:]\n",
    "# property info\n",
    "Train_df_property  = Training_set_property.iloc[:,1:]\n",
    "# Z-Score using scipy\n",
    "Train_df_property['length'] = stats.zscore(Train_df_property['length'])\n",
    "Train_df_property['MFE'] = stats.zscore(Train_df_property['MFE'])\n",
    "\n",
    "# k = 3,4,5 count the normalized and raw count of kmer\n",
    "Test_df_kmer_345,Test_df_kmer_345_rawcount = _count_kmer(Test_set_sequence,345)\n",
    "del Test_df_kmer_345['ensembl_transcript_id']\n",
    "# Structure info\n",
    "Test_df_strcutrue  = Test_set_structure.iloc[:,2:]\n",
    "# property info\n",
    "Test_df_property  = Test_set_property.iloc[:,1:]\n",
    "# Z-Score using scipy\n",
    "Train_df_property['length'] = stats.zscore(Train_df_property['length'])\n",
    "Train_df_property['MFE'] = stats.zscore(Train_df_property['MFE'])\n",
    "\n",
    "# Combine kmer and structure_info\n",
    "x_train = pd.concat([pd.DataFrame(Train_df_kmer_345.values,columns = Train_df_kmer_345.columns),Train_df_strcutrue, Train_df_property],axis = 1)\n",
    "y_train = np.array(Train_df_kmer_345.index)\n",
    "\n",
    "x_test = pd.concat([pd.DataFrame(Test_df_kmer_345.values,columns = Test_df_kmer_345.columns),Test_df_strcutrue, Test_df_property],axis = 1)\n",
    "y_test = np.array(Test_df_kmer_345.index)\n",
    "\n",
    "print(\"\\n********************** Finished Data Processing ***********************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** LightGBM ***\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"\\n*** LightGBM ***\")\n",
    "\n",
    "# LightGBM params\n",
    "lgb_param_dict = {\n",
    "    \"learning_rate\":[0.1, 0.05, 0.02, 0.01],\n",
    "    \"num_leaves\": range(10,36,5),\n",
    "    \"max_depth\" : [2,3,4,5,10,20,40,50],\n",
    "    \"n_estimators\" : range(100,2500,100),\n",
    "    \n",
    "    \"min_child_samples\": range(1, 45, 2),\n",
    "    \"colsample_bytree\" : [i / 10 for i in range(2,11)],\n",
    "    \"metric\" : [\"binary_logloss\"],\n",
    "    \"n_jobs\":[1],\n",
    "    \"subsample\" :  [i / 10 for i in range(2, 11)],\n",
    "    \"subsample_freq\" : [0, 1, 2],\n",
    "    \"reg_alpha\" : [0, 0.001, 0.005, 0.01, 0.1],\n",
    "    \"reg_lambda\" : [0, 0.001, 0.005, 0.01, 0.1],\n",
    "    \"objective\":[\"binary\"],\n",
    "    \"random_state\":[SEED]\n",
    "}\n",
    "\n",
    "#Initiate model\n",
    "lgb_model = lgb.LGBMClassifier()\n",
    "#Adjust hyper-parameters with 5-fold cross validation\n",
    "lgb_rscv = RandomizedSearchCV(lgb_model, lgb_param_dict, n_iter=1000,cv = 5,verbose = 0,\n",
    "                          scoring = \"roc_auc\",random_state=SEED,n_jobs = 30)\n",
    "lgb_rscv.fit(x_train, y_train)   \n",
    "\n",
    "\n",
    "#Evaluate best LightGBM model\n",
    "#Output path\n",
    "path = os.path.join(output_dir,\"LightGBM\")\n",
    "if not (os.path.exists(path)):\n",
    "    os.mkdir(path)\n",
    "    \n",
    "# Model performance(AUROC) on cross-validation dataset\n",
    "lgb_cv_perf = np.array([ lgb_rscv.cv_results_[\"split%s_test_score\"%str(i)] for i in range(5)])[:,lgb_rscv.best_index_]\n",
    "\n",
    "#Get best model with score [max(mean(auc(5 cross validation)))]\n",
    "lgb_best_model = lgb_rscv.best_estimator_\n",
    "#Get predict_class(y_pred) and predict_probality_for_case(y_prob) of TestSet\n",
    "y_pred = lgb_best_model.predict(x_test)\n",
    "y_prob = lgb_best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "#Get model performance\n",
    "model_perf = evaluate_performance(y_test,y_pred,y_prob)\n",
    "#Output result of evaluation\n",
    "eval_output(model_perf,path)\n",
    "#You can make bar plot consisted of accuracy,sensitivity,specificity,auroc,f1 score,MCC,precision,recall,auprc according to the \"Evaluate_Result_TestSet.txt\"\n",
    "# Plot AUROC\n",
    "plot_AUROC(model_perf,path)\n",
    "\n",
    "#save model\n",
    "joblib.dump(lgb_best_model,os.path.join(path,\"best_LightGBM_model.pkl\"))\n",
    "#load model\n",
    "#lgb_best_model = joblib.load(os.path.join(path,\"best_LightGBM_model.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Output performance on cross-validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_CV_result = pd.DataFrame({\"lgb_structure_property\":lgb_cv_perf})\n",
    "ML_CV_result.to_csv(os.path.join(output_dir,\"ML_CV_result.tsv\"),sep = '\\t',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "###   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
