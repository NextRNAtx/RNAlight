{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fallen-opera",
   "metadata": {},
   "source": [
    "# 1. Predict Test-mRNA with iLoc-mRNA \n",
    "- According to the README of iLoc-mRNA, run command as follow:\n",
    "   - python  iLoc-mRNA.py  ../../../../06_Halo_seq_mRNA_independent.fa predict_result_Halo_seq_mRNA.txt\n",
    "- grep \">\" ./predict_result_Halo_seq_mRNA.txt > Halo_seq_mRNA_ref.temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-brother",
   "metadata": {},
   "source": [
    "*** \n",
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-rehabilitation",
   "metadata": {},
   "source": [
    "# 2. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bigger-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "parallel-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance of model\n",
    "def evaluate_performance(y_test, y_pred, y_prob):\n",
    "    # AUROC\n",
    "    auroc = metrics.roc_auc_score(y_test,y_prob)\n",
    "    auroc_curve = metrics.roc_curve(y_test, y_prob)\n",
    "    # AUPRC\n",
    "    auprc=metrics.average_precision_score(y_test, y_prob) \n",
    "    auprc_curve=metrics.precision_recall_curve(y_test, y_prob)\n",
    "    #Accuracy\n",
    "    accuracy=metrics.accuracy_score(y_test,y_pred) \n",
    "    #MCC\n",
    "    mcc=metrics.matthews_corrcoef(y_test,y_pred)\n",
    "    \n",
    "    recall=metrics.recall_score(y_test, y_pred)\n",
    "    precision=metrics.precision_score(y_test, y_pred)\n",
    "    f1=metrics.f1_score(y_test, y_pred)\n",
    "    class_report=metrics.classification_report(y_test, y_pred,target_names = [\"control\",\"case\"])\n",
    "\n",
    "    model_perf = {\"auroc\":auroc,\"auroc_curve\":auroc_curve,\n",
    "                  \"auprc\":auprc,\"auprc_curve\":auprc_curve,\n",
    "                  \"accuracy\":accuracy, \"mcc\": mcc,\n",
    "                  \"recall\":recall,\"precision\":precision,\"f1\":f1,\n",
    "                  \"class_report\":class_report}\n",
    "        \n",
    "    return model_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "modified-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output result of evaluation\n",
    "def eval_output(model_perf,path):\n",
    "    with open(os.path.join(path,\"Evaluate_Result_Halo_seq.txt\"),'w') as f:\n",
    "        f.write(\"AUROC=%s\\tAUPRC=%s\\tAccuracy=%s\\tMCC=%s\\tRecall=%s\\tPrecision=%s\\tf1_score=%s\\n\" %\n",
    "               (model_perf[\"auroc\"],model_perf[\"auprc\"],model_perf[\"accuracy\"],model_perf[\"mcc\"],model_perf[\"recall\"],model_perf[\"precision\"],model_perf[\"f1\"]))\n",
    "        f.write(\"\\n######NOTE#######\\n\")\n",
    "        f.write(\"#According to help_documentation of sklearn.metrics.classification_report:in binary classification, recall of the positive class is also known as sensitivity; recall of the negative class is specificity#\\n\\n\")\n",
    "        f.write(model_perf[\"class_report\"])\n",
    "        roc_auc = model_perf[\"auroc\"]\n",
    "    # AUROC info\n",
    "    fpr,tpr,threshold = model_perf[\"auroc_curve\"]\n",
    "    #return AUROC info\n",
    "    temp_df = pd.DataFrame({\"FPR\":fpr,\"TPR\":tpr})\n",
    "    temp_df.to_csv(os.path.join(path,\"AUROC_info.txt\"),header = True,index = False, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "outer-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label: {'1':\"Cytosol/Cytoplasm\",'2':\"Ribosome\",'3':\"Endoplasmic reticulum\",'4':\"Nucleus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "relevant-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"ensemble_transcript_id\" , \"True_Label\" , \"Pre_Label\", \"Cyto_Score\" , \"Nuc_Score\"]\n",
    "df_dict = {\"ensemble_transcript_id\" : [], \"True_Label\" : [], \"Pre_Label\" : [], \"Cyto_Score\" : [], \"Nuc_Score\" : []}\n",
    "\n",
    "# Get ensembl transcript ids and true lables of mRNAs\n",
    "lnc_ref = \"./Halo_seq_mRNA_ref.temp\"\n",
    "with open(lnc_ref,'r') as f1:\n",
    "    lnc_info = f1.readlines()\n",
    "    lnc_num = len(lnc_info)\n",
    "    for i in range(lnc_num):\n",
    "        fasta_name = lnc_info[i].strip().split('\\t')[1]\n",
    "        #Seq_Name and True_Label\n",
    "        df_dict[\"ensemble_transcript_id\"].append(fasta_name.split('_')[0][1:])\n",
    "        df_dict[\"True_Label\"].append(fasta_name.split('_')[1])\n",
    "\n",
    "# Get predict labels of lncRNAs by iLoc-mRNA\n",
    "label_porb = \"res.txt\"\n",
    "pre_result = pd.read_csv(label_porb,sep = ' ',index_col = False)\n",
    "lnc_num = len(pre_result)\n",
    "for i in range(lnc_num):\n",
    "    #Nuc_Score\n",
    "    nuc_score = float(pre_result.loc[i][\"4\"])\n",
    "    df_dict[\"Nuc_Score\"].append(nuc_score)\n",
    "    #Cyto_Score\n",
    "    cyto_score = 1 - nuc_score\n",
    "    df_dict[\"Cyto_Score\"].append(cyto_score)\n",
    "    #Pre_Label\n",
    "    if cyto_score >= nuc_score:\n",
    "            df_dict[\"Pre_Label\"].append(0)\n",
    "    else:\n",
    "            df_dict[\"Pre_Label\"].append(1)\n",
    "            \n",
    "outcome = pd.DataFrame(df_dict,columns = columns)\n",
    "outcome[[\"True_Label\"]] = outcome[[\"True_Label\"]].astype(int)\n",
    "outcome[[\"Pre_Label\"]] = outcome[[\"Pre_Label\"]].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sharing-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output performance of iLoc-mRNA\n",
    "path = \"./Evaluation_Result\"\n",
    "if not (os.path.exists(path)):\n",
    "    os.mkdir(path)\n",
    "model_perf = evaluate_performance(outcome[\"True_Label\"],outcome[\"Pre_Label\"],outcome[\"Nuc_Score\"])\n",
    "eval_output(model_perf,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "declared-sleep",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/picb/rnomics3/yuanguohua/anaconda3/envs/pytorch/lib/python3.6/site-packages/pandas/core/frame.py:3027: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "outcome_df = outcome.iloc[:,[0,1,2]]\n",
    "outcome_df.rename(columns={\"True_Label\":\"tag\",\"Pre_Label\":\"predict_label\"},inplace = True)\n",
    "outcome_df.to_csv(os.path.join(path,\"mRNA_sublocation_Halo_seq_iLoc-mRNA_predict.tsv\"),sep = '\\t',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-holly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
