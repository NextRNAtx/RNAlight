{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "amazing-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import collections\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.externals import joblib\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "molecular-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of k-mer in each RNA sequence\n",
    "#k-mer was normalized by total k-mer count of each RNA sequence\n",
    "def _count_kmer(Dataset,k): # k = 3,4,5,6,7 & length info\n",
    "    \n",
    "    # copy dataset\n",
    "    dataset = copy.deepcopy(Dataset)\n",
    "    # alphbet of nucleotide\n",
    "    nucleotide = ['A','C','G','T']\n",
    "    \n",
    "    # generate k-mers\n",
    "    #  k == 7:\n",
    "    seven = list(itertools.product(nucleotide,repeat=7))\n",
    "    sevenmer = []\n",
    "    for n in seven:\n",
    "        sevenmer.append(\"\".join(n))\n",
    "    \n",
    "    #  k == 6:\n",
    "    six = list(itertools.product(nucleotide,repeat=6))\n",
    "    hexamer = []\n",
    "    for n in six:\n",
    "        hexamer.append(\"\".join(n))\n",
    "\n",
    "    #  k == 5:\n",
    "    five = list(itertools.product(nucleotide,repeat=5))\n",
    "    pentamer = []\n",
    "    for n in five:\n",
    "        pentamer.append(\"\".join(n))\n",
    "    \n",
    "    #  k == 4:\n",
    "    four = list(itertools.product(nucleotide,repeat=4))\n",
    "    tetramer = []\n",
    "    for n in four:\n",
    "        tetramer.append(\"\".join(n))\n",
    "\n",
    "    # k == 3:\n",
    "    three = list(itertools.product(nucleotide,repeat=3))\n",
    "    threemer = []\n",
    "    for n in three:\n",
    "        threemer.append(\"\".join(n))\n",
    "    \n",
    "    # input features can be combinations of diffrent k values\n",
    "    if k == 34:\n",
    "        table_kmer = dict.fromkeys(threemer,0)\n",
    "        table_kmer.update(dict.fromkeys(tetramer,0))\n",
    "    if k == 45:\n",
    "        table_kmer = dict.fromkeys(tetramer,0)\n",
    "        table_kmer.update(dict.fromkeys(pentamer,0))\n",
    "    if k == 56:\n",
    "        table_kmer = dict.fromkeys(pentamer,0)\n",
    "        table_kmer.update(dict.fromkeys(hexamer,0))\n",
    "    if k == 345:\n",
    "        table_kmer = dict.fromkeys(threemer,0)\n",
    "        table_kmer.update(dict.fromkeys(tetramer,0))\n",
    "        table_kmer.update(dict.fromkeys(pentamer,0))\n",
    "    if k == 456:\n",
    "        table_kmer = dict.fromkeys(tetramer,0)\n",
    "        table_kmer.update(dict.fromkeys(pentamer,0))\n",
    "        table_kmer.update(dict.fromkeys(hexamer,0))\n",
    "    if k == 7:\n",
    "        table_kmer = dict.fromkeys(sevenmer,0)\n",
    "\n",
    "    # count k-mer for each sequence\n",
    "    for mer in table_kmer.keys():\n",
    "        table_kmer[mer] = dataset[\"cdna\"].apply(lambda x : x.count(mer))\n",
    "    \n",
    "    # new series of length\n",
    "    length = dataset[\"cdna\"].apply(len)\n",
    "    length.name = \"length\"\n",
    "    # for k-mer raw count without normalization, index: nuc:1 or cyto:0\n",
    "    rawcount_kmer_df = pd.DataFrame(table_kmer)\n",
    "    df1_rawcount = pd.concat([rawcount_kmer_df,dataset[\"ensembl_transcript_id\"],length],axis = 1)\n",
    "    df1_rawcount.index = dataset[\"tag\"]\n",
    "\n",
    "    # for k-mer frequency with normalization , index: nuc:1 or cyto:0\n",
    "    freq_kmer_df = rawcount_kmer_df.apply(lambda x: x/x.sum(),axis=1)\n",
    "    df1 = pd.concat([freq_kmer_df,dataset[\"ensembl_transcript_id\"],length],axis = 1)\n",
    "    df1.index = dataset[\"tag\"]\n",
    "\n",
    "    return df1,df1_rawcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceramic-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate performance of model\n",
    "def evaluate_performance(y_test, y_pred, y_prob):\n",
    "    # AUROC\n",
    "    auroc = metrics.roc_auc_score(y_test,y_prob)\n",
    "    auroc_curve = metrics.roc_curve(y_test, y_prob)\n",
    "    # AUPRC\n",
    "    auprc=metrics.average_precision_score(y_test, y_prob) \n",
    "    auprc_curve=metrics.precision_recall_curve(y_test, y_prob)\n",
    "    #Accuracy\n",
    "    accuracy=metrics.accuracy_score(y_test,y_pred) \n",
    "    #MCC\n",
    "    mcc=metrics.matthews_corrcoef(y_test,y_pred)\n",
    "    \n",
    "    recall=metrics.recall_score(y_test, y_pred)\n",
    "    precision=metrics.precision_score(y_test, y_pred)\n",
    "    f1=metrics.f1_score(y_test, y_pred)\n",
    "    class_report=metrics.classification_report(y_test, y_pred,target_names = [\"control\",\"case\"])\n",
    "\n",
    "    model_perf = {\"auroc\":auroc,\"auroc_curve\":auroc_curve,\n",
    "                  \"auprc\":auprc,\"auprc_curve\":auprc_curve,\n",
    "                  \"accuracy\":accuracy, \"mcc\": mcc,\n",
    "                  \"recall\":recall,\"precision\":precision,\"f1\":f1,\n",
    "                  \"class_report\":class_report}\n",
    "        \n",
    "    return model_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "retained-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output result of evaluation\n",
    "def eval_output(model_perf,path):\n",
    "    with open(os.path.join(path,\"Evaluate_Result_TestSet.txt\"),'w') as f:\n",
    "        f.write(\"AUROC=%s\\tAUPRC=%s\\tAccuracy=%s\\tMCC=%s\\tRecall=%s\\tPrecision=%s\\tf1_score=%s\\n\" %\n",
    "               (model_perf[\"auroc\"],model_perf[\"auprc\"],model_perf[\"accuracy\"],model_perf[\"mcc\"],model_perf[\"recall\"],model_perf[\"precision\"],model_perf[\"f1\"]))\n",
    "        f.write(\"\\n######NOTE#######\\n\")\n",
    "        f.write(\"#According to help_documentation of sklearn.metrics.classification_report:in binary classification, recall of the positive class is also known as sensitivity; recall of the negative class is specificity#\\n\\n\")\n",
    "        f.write(model_perf[\"class_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "naval-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUROC of model\n",
    "def plot_AUROC(model_perf,path):\n",
    "    #get AUROC,FPR,TPR and threshold\n",
    "    roc_auc = model_perf[\"auroc\"]\n",
    "    fpr,tpr,threshold = model_perf[\"auroc_curve\"]\n",
    "    #return AUROC info\n",
    "    temp_df = pd.DataFrame({\"FPR\":fpr,\"TPR\":tpr})\n",
    "    temp_df.to_csv(os.path.join(path,\"AUROC_info.txt\"),header = True,index = False, sep = '\\t')\n",
    "    #plot\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='AUROC (area = %0.2f)' % roc_auc) \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"AUROC of Models\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(os.path.join(path,\"AUROC_TestSet.pdf\"),format = \"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "premium-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# Output dir\n",
    "output_dir = \"Evaluation_Result\"\n",
    "if not (os.path.exists(output_dir)):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data set\n",
    "TestSet = pd.read_csv(\"../../Datasets/lncRNA_sublocation_TestSet.tsv\",sep = '\\t')\n",
    "TestSet_df_kmer_345,TestSet_df_kmer_345_rawcount = _count_kmer(TestSet,345)\n",
    "\n",
    "del TestSet_df_kmer_345['ensembl_transcript_id']\n",
    "del TestSet_df_kmer_345['length']\n",
    "\n",
    "# Load LncLight model\n",
    "lgb_best_model = joblib.load(\"../../ML_original_lncRNA/20210208_cv_test/LightGBM/best_LightGBM_model.pkl\")\n",
    "\n",
    "# Get predict_class(y_pred) and predict_probality_for_case(y_prob) of TestSet\n",
    "y_pred = lgb_best_model.predict(TestSet_df_kmer_345)\n",
    "y_prob = lgb_best_model.predict_proba(TestSet_df_kmer_345)[:,1]\n",
    "\n",
    "# Evaluate model performance\n",
    "model_perf = evaluate_performance(np.array(TestSet_df_kmer_345.index),y_pred,y_prob)\n",
    "eval_output(model_perf,output_dir)\n",
    "plot_AUROC(model_perf,output_dir)\n",
    "\n",
    "# Add predict label\n",
    "TestSet[\"predict_label\"] = y_pred\n",
    "TestSet.to_csv(os.path.join(output_dir,\"lncRNA_sublocation_TestSet_LncLight_predict.tsv\"),sep = '\\t',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-memory",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
